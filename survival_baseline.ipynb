{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdf0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import CoxTimeVaryingFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import config\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest,GradientBoostingSurvivalAnalysis\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sksurv.functions import StepFunction\n",
    "\n",
    "\n",
    "import torch \n",
    "import torchtuples as tt \n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import LogisticHazard, DeepHitSingle, PMF, CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "\n",
    "from sksurv.metrics import integrated_brier_score                                           \n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from preprocess.impute import impute_data\n",
    "from utils import *\n",
    "import os\n",
    "from skimage.exposure import match_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.COLUMNS_CLASS, 'r') as yaml_file:\n",
    "    column_class = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "real_baseline_df = pd.read_parquet(config.BASELINE_COMBINE_FILE).reset_index()\n",
    "real_baseline_df = real_baseline_df.drop(columns=['admit'], errors='ignore')\n",
    "\n",
    "impute_method = \"add_missing\"\n",
    "apply_histogram_equalize = False\n",
    "\n",
    "cgan_df = pd.read_csv(config.CGAN_SYNTHETIC_BASELINE + impute_method + '.csv')\n",
    "survadgan_df =  pd.read_csv(config.SURVADGAN_SYNTHETIC_BASELINE + impute_method + '.csv')\n",
    "tvae_df = pd.read_csv(config.TVAE_SYNTHETIC_BASELINE + impute_method + '.csv')\n",
    "adgan_df = pd.read_csv(config.ADGAN_SYNTHETIC_BASELINE + impute_method + '.csv')\n",
    "ddpm_df = pd.read_csv(config.DDPM_SYNTHETIC_BASELINE + impute_method + '.csv')\n",
    "nflow_df = pd.read_csv(config.NFLOW_SYNTHETIC_BASELINE + impute_method + '.csv')\n",
    "\n",
    "real_train_idx = np.load(config.INDEX_TRAIN )\n",
    "real_val_idx = np.load(config.INDEX_VAL)\n",
    "real_test_idx = np.load(config.INDEX_TEST )\n",
    "\n",
    "survadgan_train_idx = np.load(config.SURVADGAN_INDEX_TRAIN+ impute_method + '.npy')\n",
    "survadgan_val_idx = np.load(config.SURVADGAN_INDEX_VAL+ impute_method + '.npy')\n",
    "survadgan_test_idx = np.load(config.SURVADGAN_INDEX_TEST+ impute_method + '.npy')\n",
    "\n",
    "cgan_train_idx = np.load(config.CGAN_INDEX_TRAIN+ impute_method + '.npy')\n",
    "cgan_val_idx = np.load(config.CGAN_INDEX_VAL+ impute_method + '.npy')\n",
    "cgan_test_idx = np.load(config.CGAN_INDEX_TEST+ impute_method + '.npy')\n",
    "\n",
    "tvae_train_idx = np.load(config.TVAE_INDEX_TRAIN+ impute_method + '.npy')\n",
    "tvae_val_idx = np.load(config.TVAE_INDEX_VAL+ impute_method + '.npy')\n",
    "tvae_test_idx = np.load(config.TVAE_INDEX_TEST+ impute_method + '.npy')\n",
    "\n",
    "adgan_train_idx = np.load(config.ADGAN_INDEX_TRAIN+ impute_method + '.npy')\n",
    "adgan_val_idx = np.load(config.ADGAN_INDEX_VAL+ impute_method + '.npy')\n",
    "adgan_test_idx = np.load(config.ADGAN_INDEX_TEST+ impute_method + '.npy')\n",
    "\n",
    "ddpm_train_idx = np.load(config.DDPM_INDEX_TRAIN+ impute_method + '.npy')\n",
    "ddpm_val_idx = np.load(config.DDPM_INDEX_VAL+ impute_method + '.npy')\n",
    "ddpm_test_idx = np.load(config.DDPM_INDEX_TEST+ impute_method + '.npy')\n",
    "\n",
    "nflow_train_idx = np.load(config.NFLOW_INDEX_TRAIN+ impute_method + '.npy')\n",
    "nflow_val_idx = np.load(config.NFLOW_INDEX_VAL+ impute_method + '.npy')\n",
    "nflow_test_idx = np.load(config.NFLOW_INDEX_TEST+ impute_method + '.npy')\n",
    "\n",
    "cgan_df.columns = [col.replace('_1', '') for col in cgan_df.columns]\n",
    "tvae_df.columns = [col.replace('_1', '') for col in tvae_df.columns]\n",
    "survadgan_df.columns = [col.replace('_1', '') for col in survadgan_df.columns]\n",
    "adgan_df.columns = [col.replace('_1', '') for col in adgan_df.columns]\n",
    "ddpm_df.columns = [col.replace('_1', '') for col in ddpm_df.columns]\n",
    "nflow_df.columns = [col.replace('_1', '') for col in nflow_df.columns]\n",
    "\n",
    "column_order = real_baseline_df.columns.tolist()\n",
    "cgan_df = cgan_df[column_order]\n",
    "tvae_df = tvae_df[column_order]\n",
    "survadgan_df = survadgan_df[column_order]\n",
    "adgan_df = adgan_df[column_order]\n",
    "ddpm_df = ddpm_df[column_order]\n",
    "nflow_df = nflow_df[column_order]\n",
    "\n",
    "if apply_histogram_equalize:\n",
    "    use_index = np.concatenate([real_train_idx, real_val_idx])\n",
    "\n",
    "    adgan_df['Days'] = match_histograms(adgan_df['Days'].to_numpy(), real_baseline_df['Days'][real_baseline_df['ENC_HN'].isin(use_index)].to_numpy())\n",
    "    cgan_df['Days'] = match_histograms(cgan_df['Days'].to_numpy(), real_baseline_df['Days'][real_baseline_df['ENC_HN'].isin(use_index)].to_numpy())\n",
    "    survadgan_df['Days'] = match_histograms(survadgan_df['Days'].to_numpy(), real_baseline_df['Days'][real_baseline_df['ENC_HN'].isin(use_index)].to_numpy())\n",
    "    tvae_df['Days'] = match_histograms(tvae_df['Days'].to_numpy(), real_baseline_df['Days'][real_baseline_df['ENC_HN'].isin(use_index)].to_numpy())\n",
    "    ddpm_df['Days'] = match_histograms(ddpm_df['Days'].to_numpy(), real_baseline_df['Days'][real_baseline_df['ENC_HN'].isin(use_index)].to_numpy())\n",
    "    nflow_df['Days'] = match_histograms(nflow_df['Days'].to_numpy(), real_baseline_df['Days'][real_baseline_df['ENC_HN'].isin(use_index)].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6c3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Check continuous column to normalized\n",
    "continuous_cols = [col for col in column_class['continuous'] if (col != 'Days') and col in real_baseline_df.columns]\n",
    "log_transform_cols = []\n",
    "\n",
    "def preprocess_data(df, train_idx, val_idx, test_idx, continuous_cols=None, \n",
    "                   log_transform_cols=None, verbose=True):\n",
    "    \n",
    "    train_df = df.loc[train_idx].copy()\n",
    "    val_df = df.loc[val_idx].copy() \n",
    "    test_df = df.loc[test_idx].copy()\n",
    "\n",
    "    if log_transform_cols and continuous_cols:\n",
    "        if verbose:\n",
    "            print(f\"Applying log transformation to: {log_transform_cols}\")\n",
    "        for col in log_transform_cols:\n",
    "            if col in continuous_cols:\n",
    "                train_df[col] = np.log(train_df[col] + 1e-8)\n",
    "                val_df[col] = np.log(val_df[col] + 1e-8)\n",
    "                test_df[col] = np.log(test_df[col] + 1e-8)\n",
    "    \n",
    "    if continuous_cols:\n",
    "        scaler = StandardScaler()\n",
    "        train_df[continuous_cols] = scaler.fit_transform(train_df[continuous_cols])\n",
    "        val_df[continuous_cols] = scaler.transform(val_df[continuous_cols])\n",
    "        test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Data preprocessing completed.\")\n",
    "        print(f\"Training set shape: {train_df.shape}\")\n",
    "        print(f\"Validation set shape: {val_df.shape}\")\n",
    "        print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "    for df_split in [train_df, val_df, test_df]:\n",
    "        for col in ['dead', 'admit']:\n",
    "            if col in df_split.columns:\n",
    "                df_split[col] = df_split[col].astype(bool)\n",
    "    \n",
    "    return train_df, val_df, test_df, scaler\n",
    "\n",
    "def tune_hyperparameters(model_class, param_grid, train_data, val_data, \n",
    "                        scoring_func, fit_func, predict_func, maximize=True, verbose=True):\n",
    "    \n",
    "    best_score = -np.inf if maximize else np.inf\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    for param_combo in product(*param_values):\n",
    "        params = dict(zip(param_names, param_combo))\n",
    "\n",
    "        try:\n",
    "            model = model_class(**params)\n",
    "            fit_func(model, train_data)\n",
    "            predictions = predict_func(model, val_data)\n",
    "            score = scoring_func(val_data, predictions)\n",
    "\n",
    "            results.append({**params, \"score\": score})\n",
    "            if (maximize and score > best_score) or (not maximize and score < best_score):\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                if verbose:\n",
    "                    param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "                    # print(f\"{param_str} -> score={score:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "                # print(f\"Failed for {param_str} -> {e}\")\n",
    "            results.append({**params, \"score\": np.nan, \"error\": str(e)})\n",
    "\n",
    "    return best_params, best_score, results\n",
    "\n",
    "def save_plot(model, model_label, train_label, event_col, out_dir):\n",
    "    from pathlib import Path\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filename = f\"{train_label}.png\"\n",
    "    full_path = Path(out_dir) / filename\n",
    "\n",
    "    ax = model.plot()\n",
    "    ax.set_title(f\"{model_label}: train {train_label} ({event_col})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(full_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {full_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f58e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival Analysis Libraries\n",
    "\n",
    "def run_lifelines(train_df, val_df, test_df, event_col, model_type, param_grid, train_label, final_model = None):\n",
    "\n",
    "    def fit_func(model, data):\n",
    "        model.fit(data, duration_col='Days',event_col=event_col)\n",
    "\n",
    "    def predict_func(model, data):\n",
    "        return model.predict_partial_hazard(data)\n",
    "\n",
    "    def scoring_func(data, predictions):\n",
    "        return concordance_index(data['Days'], -predictions, data[event_col])\n",
    "    \n",
    "    if final_model is None:\n",
    "\n",
    "        best_params, _, _ = tune_hyperparameters(model_type, param_grid, train_df, val_df, fit_func=fit_func, predict_func=predict_func, scoring_func=scoring_func)\n",
    "\n",
    "\n",
    "        final_model = model_type(**best_params)\n",
    "\n",
    "        final_model.fit(pd.concat([train_df, val_df]), 'Days', event_col)\n",
    "    \n",
    "    test_pred = final_model.predict_partial_hazard(test_df)\n",
    "    c_index = concordance_index(test_df['Days'], -test_pred, test_df[event_col])\n",
    "    \n",
    "    lower, upper = np.percentile(test_df['Days'], [10, 90])\n",
    "    times  = np.arange(lower, upper + 1)   \n",
    "    y_train = Surv.from_dataframe(event_col, \"Days\", train_df)\n",
    "    y_test  = Surv.from_dataframe(event_col, \"Days\", test_df)\n",
    "    surv_mat = (final_model.predict_survival_function(test_df, times=times).values.T  )\n",
    "\n",
    "    ibs = integrated_brier_score(y_train, y_test, surv_mat, times)\n",
    "\n",
    "    return {\"c_index\": c_index, \"ibs\": ibs}, final_model\n",
    "\n",
    "def run_sksurv(train_df, val_df, test_df, event_col, model_type, param_grid, train_label, final_model = None):\n",
    "\n",
    "    def _xy(df):\n",
    "        X = df.drop([\"Days\", event_col], axis=1)\n",
    "        y = Surv.from_dataframe(event_col, \"Days\", df)\n",
    "        return X, y\n",
    "\n",
    "    X_train, y_train = _xy(train_df)\n",
    "    X_val,   y_val   = _xy(val_df)\n",
    "    X_test,  y_test  = _xy(test_df)\n",
    "\n",
    "    def fit_func(model, data):\n",
    "        X, y = _xy(data)\n",
    "        model.fit(X, y)\n",
    "\n",
    "    def predict_func(model, data):\n",
    "        X, _ = _xy(data)\n",
    "        return model.predict(X)\n",
    "\n",
    "    def scoring_func(data, predictions):\n",
    "        _, y = _xy(data)\n",
    "        return concordance_index_censored(y[event_col], y['Days'], predictions)[0]\n",
    "    \n",
    "    if final_model is None:\n",
    "\n",
    "        best_params, _, _ = tune_hyperparameters(model_type, param_grid, train_df, val_df, fit_func=fit_func, predict_func=predict_func, scoring_func=scoring_func)\n",
    "\n",
    "        final_model = model_type(**best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "    c_index = final_model.score(X_test, y_test)\n",
    "    lower, upper = np.percentile(test_df['Days'], [10, 90])\n",
    "    times  = np.arange(lower, upper + 1)               \n",
    "    surv_mat = np.vstack([fn(times)             \n",
    "                          for fn in final_model.predict_survival_function(X_test)])\n",
    "    ibs  = integrated_brier_score(y_train, y_test, surv_mat, times)\n",
    "    return {\"c_index\": c_index, \"ibs\": ibs}, final_model\n",
    "\n",
    "def run_pycox(train_df, val_df, test_df, event_col, model_type, train_label, lr = 0.0001, n_discretize=50, final_model = None):\n",
    "    train_df = deepcopy(train_df)\n",
    "    val_df = deepcopy(val_df)\n",
    "    test_df = deepcopy(test_df)\n",
    "\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['Days', event_col]]\n",
    "\n",
    "    x_mapper = DataFrameMapper([(col, None) for col in feature_cols])\n",
    "\n",
    "    x_train = x_mapper.fit_transform(train_df).astype('float32')\n",
    "    x_val = x_mapper.transform(val_df).astype('float32')\n",
    "    x_test = x_mapper.transform(test_df).astype('float32')\n",
    "\n",
    "    get_target = lambda d: (d['Days'].values.astype('float32'), \n",
    "                          d[event_col].values.astype('float32'))\n",
    "    durations_test, events_test = get_target(test_df)\n",
    "\n",
    "    if final_model is None:\n",
    "        try:\n",
    "            labtrans = model_type.label_transform(n_discretize)\n",
    "            y_train = labtrans.fit_transform(*get_target(train_df))\n",
    "            y_val = labtrans.transform(*get_target(val_df))\n",
    "            out_features = labtrans.out_features\n",
    "            net = tt.practical.MLPVanilla(x_train.shape[1], [x_train.shape[1]*3, x_train.shape[1]*5, x_train.shape[1]*3], out_features, \n",
    "                                    batch_norm=True, dropout=0.5)\n",
    "            final_model = model_type(net, tt.optim.Adam(lr), duration_index=labtrans.cuts, alpha = 0.1)\n",
    "        except Exception as e:\n",
    "            y_train = get_target(train_df)\n",
    "            y_val = get_target(val_df)\n",
    "            out_features = 1\n",
    "\n",
    "            net = tt.practical.MLPVanilla(x_train.shape[1], [x_train.shape[1]*3, x_train.shape[1]*5, x_train.shape[1]*3], out_features, \n",
    "                                    batch_norm=True, dropout=0.5)\n",
    "\n",
    "            final_model = model_type(net, tt.optim.Adam(lr))\n",
    "\n",
    "        \n",
    "\n",
    "        final_model.fit(x_train, y_train, batch_size=256, epochs=1000,\n",
    "                callbacks=[tt.cb.EarlyStopping(patience = 50)],\n",
    "                val_data=(x_val, y_val), verbose=False)    \n",
    "\n",
    "    if isinstance(final_model, CoxPH):\n",
    "        final_model.compute_baseline_hazards()\n",
    "    surv = final_model.predict_surv_df(x_test)\n",
    "    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "\n",
    "    grid = surv.index\n",
    "    lower, upper = np.percentile(test_df[\"Days\"], [10, 90])\n",
    "    times = np.asarray(grid[(grid >= lower) & (grid < upper)])\n",
    "\n",
    "    if len(times) == 0:\n",
    "        times = grid\n",
    "\n",
    "    ibs   = ev.integrated_brier_score(times)  \n",
    "    return {\"c_index\": c_index, \"ibs\": ibs}, final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd02579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running imputation: median\n",
      "Running imputation: mice\n",
      "Running imputation: hyperimpute\n"
     ]
    }
   ],
   "source": [
    "#impute and preprocess\n",
    "\n",
    "event_col = 'dead'\n",
    "processed_datasets = {}\n",
    "\n",
    "impute_methods = ['median', 'mice','hyperimpute']\n",
    "\n",
    "for method in impute_methods:\n",
    "    print(f\"Running imputation: {method}\")\n",
    "    \n",
    "    def impute_and_preprocess(df, train_idx, val_idx, test_idx):\n",
    "        df_copy = df.copy()\n",
    "        df_copy = df_copy.set_index('ENC_HN', drop=True)\n",
    "\n",
    "        df_copy = impute_data(df_copy, train_idx, val_idx, test_idx, method=method, impute_feature_only=True, impute_test_set=True)\n",
    "\n",
    "        df_copy['ENC_HN'] = df_copy.index  \n",
    "        df_copy = df_copy.set_index('ENC_HN', drop=True).drop(columns=['admit'], errors='ignore')\n",
    "        \n",
    "        return preprocess_data(df_copy, train_idx, val_idx, test_idx,\n",
    "                            continuous_cols=continuous_cols,\n",
    "                            log_transform_cols=log_transform_cols, verbose=False)\n",
    "\n",
    "    processed_datasets[method] = {\n",
    "        \"real\":     impute_and_preprocess(real_baseline_df, real_train_idx, real_val_idx, real_test_idx),\n",
    "        \"cgan\":     impute_and_preprocess(cgan_df, cgan_train_idx, cgan_val_idx, cgan_test_idx),\n",
    "        \"survadgan\": impute_and_preprocess(survadgan_df, survadgan_train_idx, survadgan_val_idx, survadgan_test_idx),\n",
    "        \"adgan\":    impute_and_preprocess(adgan_df, adgan_train_idx, adgan_val_idx, adgan_test_idx),\n",
    "        \"tvae\":     impute_and_preprocess(tvae_df, tvae_train_idx, tvae_val_idx, tvae_test_idx),\n",
    "        \"ddpm\":     impute_and_preprocess(ddpm_df, ddpm_train_idx, ddpm_val_idx, ddpm_test_idx),\n",
    "        \"nflow\":    impute_and_preprocess(nflow_df, nflow_train_idx, nflow_val_idx, nflow_test_idx)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e5efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train: real  Test: real]  Lifelines CoxPH\n",
      "[Train: real  Test: adgan]  Lifelines CoxPH\n",
      "[Train: real  Test: real]  SKSurv RandomForest\n",
      "[Train: real  Test: adgan]  SKSurv RandomForest\n",
      "[Train: real  Test: real]  PyCox DeepSurv\n",
      "[Train: real  Test: adgan]  PyCox DeepSurv\n",
      "[Train: real  Test: real]  PyCox DeepHit\n",
      "[Train: real  Test: adgan]  PyCox DeepHit\n",
      "[Train: adgan  Test: adgan]  Lifelines CoxPH\n",
      "[Train: adgan  Test: real]  Lifelines CoxPH\n",
      "[Train: adgan  Test: adgan]  SKSurv RandomForest\n",
      "[Train: adgan  Test: real]  SKSurv RandomForest\n",
      "[Train: adgan  Test: adgan]  PyCox DeepSurv\n",
      "[Train: adgan  Test: real]  PyCox DeepSurv\n",
      "[Train: adgan  Test: adgan]  PyCox DeepHit\n",
      "[Train: adgan  Test: real]  PyCox DeepHit\n",
      "[Train: real  Test: real]  Lifelines CoxPH\n",
      "[Train: real  Test: adgan]  Lifelines CoxPH\n",
      "[Train: real  Test: real]  SKSurv RandomForest\n",
      "[Train: real  Test: adgan]  SKSurv RandomForest\n",
      "[Train: real  Test: real]  PyCox DeepSurv\n",
      "[Train: real  Test: adgan]  PyCox DeepSurv\n",
      "[Train: real  Test: real]  PyCox DeepHit\n",
      "[Train: real  Test: adgan]  PyCox DeepHit\n",
      "[Train: adgan  Test: adgan]  Lifelines CoxPH\n",
      "[Train: adgan  Test: real]  Lifelines CoxPH\n",
      "[Train: adgan  Test: adgan]  SKSurv RandomForest\n",
      "[Train: adgan  Test: real]  SKSurv RandomForest\n",
      "[Train: adgan  Test: adgan]  PyCox DeepSurv\n",
      "[Train: adgan  Test: real]  PyCox DeepSurv\n",
      "[Train: adgan  Test: adgan]  PyCox DeepHit\n",
      "[Train: adgan  Test: real]  PyCox DeepHit\n",
      "[Train: real  Test: real]  Lifelines CoxPH\n",
      "[Train: real  Test: adgan]  Lifelines CoxPH\n",
      "[Train: real  Test: real]  SKSurv RandomForest\n",
      "[Train: real  Test: adgan]  SKSurv RandomForest\n",
      "[Train: real  Test: real]  PyCox DeepSurv\n",
      "[Train: real  Test: adgan]  PyCox DeepSurv\n",
      "[Train: real  Test: real]  PyCox DeepHit\n",
      "[Train: real  Test: adgan]  PyCox DeepHit\n",
      "[Train: adgan  Test: adgan]  Lifelines CoxPH\n",
      "[Train: adgan  Test: real]  Lifelines CoxPH\n",
      "[Train: adgan  Test: adgan]  SKSurv RandomForest\n",
      "[Train: adgan  Test: real]  SKSurv RandomForest\n",
      "[Train: adgan  Test: adgan]  PyCox DeepSurv\n",
      "[Train: adgan  Test: real]  PyCox DeepSurv\n",
      "[Train: adgan  Test: adgan]  PyCox DeepHit\n",
      "[Train: adgan  Test: real]  PyCox DeepHit\n"
     ]
    }
   ],
   "source": [
    "# survival metrics\n",
    "\n",
    "models = [\n",
    "    (\"Lifelines CoxPH\", run_lifelines, {\n",
    "        'model_type': CoxPHFitter,\n",
    "        'param_grid': {\n",
    "            'penalizer': [ 0.1, 1],\n",
    "            'l1_ratio': [0, 0.5, 1]\n",
    "        }\n",
    "    }),\n",
    "    # # (\"SKSurv CoxPH\", run_sksurv, {\n",
    "    # #     'model_type': CoxPHSurvivalAnalysis,\n",
    "    # #     'param_grid': {'alpha': [0.1]}\n",
    "    # # }),\n",
    "    (\"SKSurv RandomForest\", run_sksurv, {\n",
    "        'model_type': RandomSurvivalForest,\n",
    "        'param_grid': {\n",
    "            'n_estimators': [5, 20, 50],\n",
    "            'max_depth': [2, 5, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'n_jobs': [-1],  # Use all CPUs\n",
    "            'random_state': [515616]\n",
    "        }\n",
    "    }),\n",
    "    (\"PyCox DeepSurv\", run_pycox, {'model_type': CoxPH}),\n",
    "    (\"PyCox DeepHit\", run_pycox, {'model_type': DeepHitSingle}),\n",
    "]\n",
    "\n",
    "dataset_names = ['real', 'cgan', 'survadgan', 'adgan', 'tvae', 'ddpm', 'nflow']\n",
    "\n",
    "def process_testdf(test_df, test_scaler, train_scaler, max_train_time):\n",
    "    test_df = deepcopy(test_df)\n",
    "    test_df[continuous_cols] = test_scaler.inverse_transform(test_df[continuous_cols])\n",
    "\n",
    "    test_df[continuous_cols] = train_scaler.transform(test_df[continuous_cols])\n",
    "    test_df['Days'] = np.minimum(test_df['Days'], max_train_time)\n",
    "    # scaler = StandardScaler()\n",
    "    # test_df[continuous_cols] = scaler.fit_transform(test_df[continuous_cols])\n",
    "    return test_df\n",
    "\n",
    "for method in impute_methods:\n",
    "    processed = processed_datasets[method]\n",
    "    survival_metrics = []\n",
    "\n",
    "    def add_result(train_name, test_name, model_name, metrics):\n",
    "        survival_metrics.append({\n",
    "            \"Train\": train_name,\n",
    "            \"Test\": test_name,\n",
    "            \"Model\": model_name,\n",
    "            \"Impute\": method,\n",
    "            \"C-Index\": metrics[\"c_index\"],\n",
    "            \"IBS\": metrics[\"ibs\"]\n",
    "        })\n",
    "\n",
    "    train_name = 'real'\n",
    "\n",
    "\n",
    "    for model_name, model_func, kwargs in models:\n",
    "        final_model = None\n",
    "        for test_name in dataset_names:\n",
    "\n",
    "            train_df, val_df, _, train_scaler = processed[train_name]\n",
    "            max_train_time = train_df['Days'].max()\n",
    "            _, _, test_df, test_scaler = processed[test_name]\n",
    "            test_df = process_testdf(test_df, test_scaler, train_scaler, max_train_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"[Train: {train_name}  Test: {test_name}]  {model_name}\")\n",
    "            try:\n",
    "                metrics, final_model = model_func(train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                                        event_col=event_col, train_label=train_name, final_model = final_model, **kwargs)\n",
    "                add_result(train_name, test_name, model_name, metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {model_name} on {train_name}-->{test_name}: {e}\")\n",
    "                raise ValueError(f\"Error in {model_name} on {train_name} to {test_name}: {e}\")\n",
    "\n",
    "    for model_name, model_func, kwargs in models:\n",
    "        for train_name in dataset_names:\n",
    "            if train_name == 'real':\n",
    "                continue\n",
    "            test_name = train_name\n",
    "            train_df, val_df, test_df, train_scaler = processed[train_name]\n",
    "            max_train_time = train_df['Days'].max()\n",
    "\n",
    "            print(f\"[Train: {train_name}  Test: {test_name}]  {model_name}\")\n",
    "            try:\n",
    "                metrics, final_model = model_func(train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                                        event_col=event_col, train_label=train_name, **kwargs)\n",
    "                add_result(train_name, test_name, model_name, metrics)\n",
    "\n",
    "                test_name = 'real'\n",
    "                print(f\"[Train: {train_name}  Test: {test_name}]  {model_name}\")\n",
    "                _, _, test_df, test_scaler = processed[test_name]\n",
    "                \n",
    "                test_df = process_testdf(test_df, test_scaler, train_scaler, max_train_time)\n",
    "\n",
    "\n",
    "                metrics, _ = model_func(train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                                        event_col=event_col, train_label=train_name, final_model=final_model,**kwargs)\n",
    "                add_result(train_name, test_name, model_name, metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {model_name} on {train_name}-->{test_name}: {e}\")\n",
    "                add_result(train_name, test_name, model_name, {\"c_index\": None, \"ibs\": None})\n",
    "\n",
    "    df = pd.DataFrame(survival_metrics)\n",
    "    df[\"Train Test\"] = \"Train \" + df[\"Train\"].str.capitalize() + \" --> Test \" + df[\"Test\"].str.capitalize()\n",
    "\n",
    "    cindex = df.pivot(index=\"Train Test\", columns=\"Model\", values=\"C-Index\")\n",
    "    ibs = df.pivot(index=\"Train Test\", columns=\"Model\", values=\"IBS\")\n",
    "\n",
    "\n",
    "    out_folder = f\"survival_results_{method}\"\n",
    "    \n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    with pd.ExcelWriter(f\"{out_folder}/survival_metrics.xlsx\") as xl:\n",
    "        cindex.to_excel(xl, sheet_name=\"C-Index\")\n",
    "        ibs.to_excel(xl, sheet_name=\"IBS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75049bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Alternative more concise version:\n",
    "def sort_train_test_dataframe_v2(df):\n",
    "    \"\"\"\n",
    "    More concise version of the sorting function with simplified index names\n",
    "    \"\"\"\n",
    "    def sort_key(idx):\n",
    "        parts = idx.split(' --> ')\n",
    "        train = parts[0].replace('Train ', '').strip().lower()\n",
    "        test = parts[1].replace('Test ', '').strip().lower()\n",
    "        \n",
    "        if train == 'real' and test == 'real':\n",
    "            return (1, train, test)\n",
    "        elif train != 'real' and test == 'real':\n",
    "            return (2, train, test)  \n",
    "        elif train == 'real' and test != 'real':\n",
    "            return (3, train, test)\n",
    "        else:\n",
    "            return (4, train, test)\n",
    "    \n",
    "    # Sort index by the custom key\n",
    "    sorted_index = sorted(df.index, key=sort_key)\n",
    "    sorted_df = df.loc[sorted_index].copy()\n",
    "    \n",
    "    # Create new simplified index names\n",
    "    new_index = []\n",
    "    for idx in sorted_df.index:\n",
    "        parts = idx.split(' --> ')\n",
    "        train = parts[0].replace('Train ', '').strip()\n",
    "        test = parts[1].replace('Test ', '').strip()\n",
    "        \n",
    "        if train.lower() == 'real' and test.lower() == 'real':\n",
    "            new_index.append('Real')\n",
    "        elif train.lower() == 'real' and test.lower() != 'real':\n",
    "            if test.upper() == 'SURVADGAN':\n",
    "                new_index.append(f'SurvivalGAN')\n",
    "            elif test.upper() == 'ADGAN':\n",
    "                new_index.append(f'ADSGAN')\n",
    "            elif test.upper() == 'CGAN':\n",
    "                new_index.append(f'CTGAN')\n",
    "            else:\n",
    "                new_index.append(f'{test.upper()}')\n",
    "        elif train.lower() != 'real' and test.lower() == 'real':\n",
    "            if train.upper() == 'SURVADGAN':\n",
    "                new_index.append(f'SurvivalGAN')\n",
    "            elif train.upper() == 'ADGAN':\n",
    "                new_index.append(f'ADSGAN')\n",
    "            elif train.upper() == 'CGAN':\n",
    "                new_index.append(f'CTGAN')\n",
    "            else:\n",
    "                new_index.append(f'{train.upper()}')\n",
    "        else:\n",
    "            if train.upper() == 'SURVADGAN':\n",
    "                new_index.append(f'SurvivalGAN')\n",
    "            elif train.upper() == 'ADGAN':\n",
    "                new_index.append(f'ADSGAN')\n",
    "            elif train.upper() == 'CGAN':\n",
    "                new_index.append(f'CTGAN')\n",
    "            else:\n",
    "                new_index.append(f'{train.upper()}')\n",
    "    \n",
    "    sorted_df.index = new_index\n",
    "    return sorted_df\n",
    "\n",
    "name = 'mice'\n",
    "df_c = pd.read_excel(f'survival_results_{name}/survival_metrics.xlsx', index_col=0, sheet_name='C-Index')\n",
    "df_ibs = pd.read_excel(f'survival_results_{name}/survival_metrics.xlsx', index_col=0, sheet_name='IBS')\n",
    "df_final = df_c.round(2).astype(str) + \" (\"+df_ibs.round(2).astype(str)+ \")\"\n",
    "df_final1 = sort_train_test_dataframe_v2(df_final)\n",
    "\n",
    "\n",
    "name = 'median'\n",
    "df_c = pd.read_excel(f'survival_results_{name}/survival_metrics.xlsx', index_col=0, sheet_name='C-Index')\n",
    "df_ibs = pd.read_excel(f'survival_results_{name}/survival_metrics.xlsx', index_col=0, sheet_name='IBS')\n",
    "df_final = df_c.round(2).astype(str) + \" (\"+df_ibs.round(2).astype(str)+ \")\"\n",
    "df_final2 = sort_train_test_dataframe_v2(df_final)\n",
    "\n",
    "\n",
    "name='hyperimpute'\n",
    "df_c = pd.read_excel(f'survival_results_{name}/survival_metrics.xlsx', index_col=0, sheet_name='C-Index')\n",
    "df_ibs = pd.read_excel(f'survival_results_{name}/survival_metrics.xlsx', index_col=0, sheet_name='IBS')\n",
    "df_final = df_c.round(2).astype(str) + \" (\"+df_ibs.round(2).astype(str)+ \")\"\n",
    "df_final3 = sort_train_test_dataframe_v2(df_final)\n",
    "\n",
    "# save add multiple sheet in excel\n",
    "with pd.ExcelWriter('results/survival_results_combined.xlsx') as writer:\n",
    "    df_final1.to_excel(writer, sheet_name='mice')\n",
    "    df_final2.to_excel(writer, sheet_name='median')\n",
    "    df_final3.to_excel(writer, sheet_name='hyperimpute')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bd195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d886b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adcc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3817a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream_synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
